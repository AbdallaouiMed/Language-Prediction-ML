{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13aaa78e-eebe-47f3-a306-97775f8e3647",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c5ebb4-48b7-4f25-a362-c8c1311b3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import librosa.effects as effects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899a2c9-3e64-483e-b2f9-d1dd0cf1657c",
   "metadata": {},
   "source": [
    "## Extract the Mfcc features and remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb42946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path = \"/Users/mohamedabdallaoui/Desktop/Langue\"\n",
    "language = \"russe\"\n",
    "genders = [\"Femmes\",\"Hommes\"]\n",
    "\n",
    "mfcc_folder_path = \"/Users/mohamedabdallaoui/Desktop/Russe_MFCC\"\n",
    "mfcc_folder_path_femmes = os.path.join(mfcc_folder_path, \"Femmes\")\n",
    "mfcc_folder_path_hommes = os.path.join(mfcc_folder_path, \"Hommes\")\n",
    "\n",
    "mfcc_features_list = [] # To store MFCC features for all audio files\n",
    "\n",
    "#This will loop through all the audio files in each gender, and load each audio file using librosa\n",
    "for gender in genders:\n",
    "    audio_files_path = os.path.join(path, language, gender)\n",
    "    audio_files = os.listdir(audio_files_path)\n",
    "    for audio_file in audio_files:\n",
    "        audio_file_path = os.path.join(audio_files_path, audio_file)\n",
    "        audio_signal, sample_rate = librosa.load(audio_file_path)\n",
    "\n",
    "        #remove silence \n",
    "        audio_signal = effects.trim(audio_signal, top_db=40)[0]\n",
    "\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfcc_features = librosa.feature.mfcc(y=audio_signal, sr=sample_rate)\n",
    "        mfcc_features_list.append(mfcc_features)\n",
    "        \n",
    "        # Save MFCC features in a new file in desktop\n",
    "        if gender == \"Hommes\":\n",
    "            for i in range(1, len(audio_files)+1):\n",
    "                if f\"h{i}.wav\" in audio_file:\n",
    "                    mfcc_file_path = os.path.join(mfcc_folder_path_hommes, f\"{language}_{gender}_h{i}.npy\")\n",
    "                    np.save(mfcc_file_path, mfcc_features)\n",
    "\n",
    "                    break\n",
    "        elif gender == \"Femmes\":\n",
    "            for i in range(1, len(audio_files)+1):\n",
    "                if f\"f{i}.wav\" in audio_file:\n",
    "                    mfcc_file_path = os.path.join(mfcc_folder_path_femmes, f\"{language}_{gender}_f{i}.npy\")\n",
    "                    np.save(mfcc_file_path, mfcc_features)\n",
    "\n",
    "                    break\n",
    "\n",
    "# Pad MFCC features with zeros to make them of equal shape\n",
    "# Maximum length of MFCC features\n",
    "max_length = np.max([mfcc_features.shape[1] for mfcc_features in mfcc_features_list]) \n",
    "padded_features_list = []\n",
    "for mfcc_features in mfcc_features_list:\n",
    "    padded_features = librosa.util.pad_center(mfcc_features, size=max_length, axis=1)\n",
    "    padded_features_list.append(padded_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b563d3",
   "metadata": {},
   "source": [
    "## Split the data into train and test and save it in theire respective files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d79cdf93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load all the MFCC files for a language and gender\n",
    "language = \"russe\"\n",
    "genders = [\"Femmes\", \"Hommes\"]\n",
    "mfcc_folder_path = \"/Users/mohamedabdallaoui/Desktop/Russe_MFCC\"\n",
    "\n",
    "mfcc_files = []\n",
    "for gender in genders:\n",
    "    audio_files_path = os.path.join(mfcc_folder_path, gender)\n",
    "    audio_files = os.listdir(audio_files_path)\n",
    "    for audio_file in audio_files:\n",
    "        if audio_file.endswith(\".npy\"):\n",
    "            mfcc_file_path = os.path.join(audio_files_path, audio_file)\n",
    "            mfcc_files.append(mfcc_file_path)\n",
    "\n",
    "# Split the MFCC files into train and test sets\n",
    "train_files, test_files = train_test_split(mfcc_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the train and test directories if they do not exist\n",
    "train_dir = \"/Users/mohamedabdallaoui/Desktop/train_russe\"\n",
    "test_dir = \"/Users/mohamedabdallaoui/Desktop/test_russe\"\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    os.makedirs(train_dir)\n",
    "\n",
    "if not os.path.exists(test_dir):\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "# Save the train and test files in their respective directories\n",
    "\n",
    "for file_path in train_files:\n",
    "    if \"russe_Femmes\" in file_path:\n",
    "        gender_folder = \"Femmes\"\n",
    "    elif \"russe_Hommes\" in file_path:\n",
    "        gender_folder = \"Hommes\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    filename = os.path.basename(file_path)\n",
    "    dest_path = os.path.join(train_dir, gender_folder, filename)\n",
    "    np.save(dest_path, np.load(file_path))\n",
    "\n",
    "for file_path in test_files:\n",
    "    if \"russe_Femmes\" in file_path:\n",
    "        gender_folder = \"Femmes\"\n",
    "    elif \"russe_Hommes\" in file_path:\n",
    "        gender_folder = \"Hommes\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    filename = os.path.basename(file_path)\n",
    "    dest_path = os.path.join(test_dir, gender_folder, filename)\n",
    "    np.save(dest_path, np.load(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495addc7-a3d9-40da-acf3-13f3c5801964",
   "metadata": {},
   "source": [
    "# Traing the model using Gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1030fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "language = \"russe\"\n",
    "genders = [\"Femmes\", \"Hommes\"]\n",
    "train_dir = \"/Users/mohamedabdallaoui/Desktop/train_russe\"\n",
    "\n",
    "# Load the train data\n",
    "train_features = []\n",
    "for gender in genders:\n",
    "    gender_train_dir = os.path.join(train_dir, gender)\n",
    "    audio_files = os.listdir(gender_train_dir)\n",
    "    for audio_file in audio_files:\n",
    "        if audio_file.endswith(\".npy\"):\n",
    "            audio_file_path = os.path.join(gender_train_dir, audio_file)\n",
    "            mfcc_features = np.load(audio_file_path)\n",
    "            train_features.append(mfcc_features)\n",
    "\n",
    "train_features = np.concatenate(train_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a18c28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture(covariance_type=&#x27;diag&#x27;, n_components=64)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianMixture</label><div class=\"sk-toggleable__content\"><pre>GaussianMixture(covariance_type=&#x27;diag&#x27;, n_components=64)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianMixture(covariance_type='diag', n_components=64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the GMM on the train data\n",
    "n_components =  64 #Number of mixture components\n",
    "gmm = GaussianMixture(n_components=n_components, covariance_type=\"diag\")\n",
    "Model = gmm.fit(train_features.T)\n",
    "Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb448d",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee83b6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/mohamedabdallaoui/Desktop/gmm_model.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "# Save the trained model to a file\n",
    "filename = '/Users/mohamedabdallaoui/Desktop/gmm_model.joblib'\n",
    "dump(Model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5f531",
   "metadata": {},
   "source": [
    "# Predict the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8121ae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/Users/mohamedabdallaoui/Desktop/langue/russe/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qj/79dx7qv13zl8gg_j7wh3c0hw0000gn/T/ipykernel_39812/3045051298.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlanguage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mgender\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mgender_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0maudio_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgender_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/Users/mohamedabdallaoui/Desktop/langue/russe/.DS_Store'"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "test_dir = \"\"\n",
    "test_dir = \"/Users/mohamedabdallaoui/Desktop/langue/russe\"\n",
    "for language in os.listdir(test_dir):\n",
    "    language_dir = os.path.join(test_dir, language)\n",
    "    for gender in os.listdir(language_dir):\n",
    "        gender_dir = os.path.join(language_dir, gender)\n",
    "        audio_files = os.listdir(gender_dir)\n",
    "        for audio_file in audio_files:\n",
    "            if audio_file.endswith(\".wav\"):\n",
    "                audio_file_path = os.path.join(gender_dir, audio_file)\n",
    "                mfcc_features = extract_mfcc_features(audio_file_path)\n",
    "                test_features.append(mfcc_features)\n",
    "\n",
    "test_features = np.concatenate(test_features, axis=1)\n",
    "\n",
    "# Predict the language\n",
    "log_likelihoods = gmm.score_samples(test_features.T)[0]\n",
    "language_idx = np.argmax(np.sum(log_likelihoods, axis=0))\n",
    "predicted_language = os.listdir(\"/path/to/l'angue'\")[language_idx]\n",
    "\n",
    "print(\"Predicted language: \", predicted_language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e545c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266985df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29962d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
